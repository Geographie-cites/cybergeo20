install.packages('cartography')
install.packages('cartography',source=T)
help(install.packages)
install.packages('cartography',type='source')
library(cartography)
help(cartography)
citation(package="cartography")
resdir='20160106_LHSDensityNW/data/'
cartography: vignette(topic = "cartography")
cartography:vignette(topic = "cartography")
vignette(topic = "cartography")
data("nuts2006")
EuropeStamen <- getTiles(spdf = nuts0.spdf, type = "stamen-watercolor")
install.packages('OpenStreetMap')
EuropeStamen <- getTiles(spdf = nuts0.spdf, type = "stamen-watercolor")
tilesLayer(EuropeStamen)
plot(nuts0.spdf, add=TRUE)
mtext(text = "Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under CC BY SA.",
side = 1, adj = 0, cex = 0.7, font = 3)
install.packages('shiny')
shiny::runApp('~/Documents/ComplexSystems/Misc/Anna/063-superzip-example')
library(maps)
library(mapdata)
library(rworldmap)
load("~/Documents/ComplexSystems/CyberGeo/cybergeo20/HyperNetwork/Models/Analysis/20160217.RData")
library(dplyr)
library(igraph)
com
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
kmin = 0
kmax = 1000  # max for common ggiant is 1088
edge_th = 200  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
com
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
thematics = list()
for(i in 1:length(V(gg))){
thematics[[V(g)$name[i]]]=com$membership[i]
}
kmin = 0
kmax = 1000  # max for common ggiant is 1088
edge_th = 100  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
com
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
com = cluster_louvain(gg)
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
edge_th = 200  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
kmin = 0
kmax = 1000  # max for common ggiant is 1088
edge_th = 100  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
kmin = 0
kmax = 1050  # max for common ggiant is 1088
edge_th = 200  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
kmax = 1020  # max for common ggiant is 1088
edge_th = 200  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
com
for(i in unique(com$membership)){show(i);show(V(g)$name[which(com$membership==i)])}
for(i in unique(com$membership)){show(i);show(V(gg)$name[which(com$membership==i)])}
kmax = 1000  # max for common ggiant is 1088
edge_th = 200  # 6218
d=degree(ggiant)
gg=induced_subgraph(ggiant,which(d>kmin&d<kmax))
gg=subgraph.edges(gg,which(E(gg)$weight>edge_th))
com = cluster_louvain(gg)
com
for(i in unique(com$membership)){show(i);show(V(gg)$name[which(com$membership==i)])}
thematics = list()
for(i in 1:length(V(gg))){
thematics[[V(g)$name[i]]]=com$membership[i]
}
# compute proba matrix
them_probas = matrix(0,length(names(keyword_dico)),length(unique(com$membership)))
for(i in 1:length(names(keyword_dico))){
if(i%%100==0){show(i)}
kwcount=0
for(kw in keyword_dico[[names(keyword_dico)[i]]]){if(kw %in% names(thematics)){
j=thematics[[kw]]
them_probas[i,j]=them_probas[i,j]+1;kwcount=kwcount+1
}}
if(kwcount>0){them_probas[i,]=them_probas[i,]/kwcount}
}
# number of articles with originality
#length(which(rowSums(them_probas)>0))
originalities=apply(them_probas,MARGIN = 1,FUN = function(r){if(sum(r)==0){return(0)}else{return(1 - sum(r^2))}})
cybindexes = c();cybresnames = c();iscyb=rep(FALSE,length(originalities))
for(cyb in cybnames){
indexes = which(names(keyword_dico)==cyb);
if(length(indexes)>0){
cybindexes=append(cybindexes,indexes[1]);
cybresnames=append(cybresnames,cyb)
iscyb[indexes[1]]=TRUE
}}
ggplot(data.frame(orig=originalities,cyb=iscyb), aes(x=orig, fill=cyb)) + geom_density(alpha=.3)
library(ggplot2)
ggplot(data.frame(orig=originalities,cyb=iscyb), aes(x=orig, fill=cyb)) + geom_density(alpha=.3)
length(which(iscyb))
sdat=as.tbl(dat)%>%group_by(iscyb)%>%summarise(mean=mean(orig))
dat=data.frame(orig=originalities,cyb=iscyb)
sdat=as.tbl(dat)%>%group_by(iscyb)%>%summarise(mean=mean(orig))
sdat=as.tbl(dat)%>%group_by(cyb)%>%summarise(mean=mean(orig))
length(which(iscyb))
dat=data.frame(orig=originalities,cyb=iscyb)
sdat=as.tbl(dat)%>%group_by(cyb)%>%summarise(mean=mean(orig))
library(ggplot2)
g=ggplot(dat, aes(x=orig, fill=cyb))
g+ geom_density(alpha=.3)+geom_vline(data=sdat, aes(xintercept=mean,  colour=cyb),
linetype="dashed", size=1)
sdat
dat
as.tbl(dat)%>%group_by(cyb)
g=ggplot(dat, aes(x=orig, fill=cyb))
g+ geom_density(alpha=.3)
cybprobas = them_probas[cybindexes,]
cybcumprobas = colSums(cybprobas)/length(which(rowSums(cybprobas)>0))
1 - sum(cybcumprobas^2)
sdat
Nb = 10000
nulljournalorigs=c()
for(i in 1:Nb){
probas = them_probas[sample.int(nrow(them_probas), size = length(cybindexes), replace = FALSE),]
cumprobas = colSums(probas)/length(which(rowSums(probas)>0))
nulljournalorigs=append(nulljournalorigs,1 - sum(cumprobas^2))
}
hist(nulljournalorigs,breaks=1000)
mean(nulljournalorigs)
head(them_probas)
i=1
neighbors(gcitation,v=cybnodes[i],mode="in")$name
gcitation
cybnodes
cybnodes=V(gcitation)[V(gcitation)$cyb==1]
cybnodes
neighbors(gcitation,v=cybnodes[i],mode="in")$name
keyword_dico[[neighbors(gcitation,v=cybnodes[i],mode="in")$name]]
cybsecorigin=c()
cybsecorigout=c()
cybsecorigall=c()
for(i in cybindexes){
show(i)
neigh = neighbors(gcitation,v=cybnodes[i],mode="in")$name
probas = rep(0,ncol(them_probas));count=0
for(n in 1:length(neigh)){
inds = which(names(keyword_dico==neigh[n]))
if(length(inds)>0){probas=probas+them_probas[inds[1],];count=count+1}
}
if(count>0){probas=probas/count}
if(sum(probas)>0){cybsecorigin=append(cybsecorigin,1-sum(probas^2))}
}
show(i)
neigh = neighbors(gcitation,v=cybnodes[i],mode="in")$name
## second order originality ?
cybsecorigin=c()
cybsecorigout=c()
cybsecorigall=c()
for(i in 1:length(cybnodes)){
show(i)
neigh = neighbors(gcitation,v=cybnodes[i],mode="in")$name
probas = rep(0,ncol(them_probas));count=0
for(n in 1:length(neigh)){
inds = which(names(keyword_dico==neigh[n]))
if(length(inds)>0){probas=probas+them_probas[inds[1],];count=count+1}
}
if(count>0){probas=probas/count}
if(sum(probas)>0){cybsecorigin=append(cybsecorigin,1-sum(probas^2))}
}
cybsecorigin=c()
cybsecorigout=c()
cybsecorigall=c()
for(i in 1:length(cybnodes)){
show(i)
neigh = neighbors(gcitation,v=cybnodes[i],mode="in")$name
show(neigh)
probas = rep(0,ncol(them_probas));count=0
for(n in 1:length(neigh)){
inds = which(names(keyword_dico==neigh[n]))
if(length(inds)>0){probas=probas+them_probas[inds[1],];count=count+1}
}
if(count>0){probas=probas/count}
if(sum(probas)>0){cybsecorigin=append(cybsecorigin,1-sum(probas^2))}
}
cybsecorigin=c()
cybsecorigout=c()
cybsecorigall=c()
for(i in 1:length(cybnodes)){
show(i)
neigh = neighbors(gcitation,v=cybnodes[i],mode="in")$name
show(neigh)
probas = rep(0,ncol(them_probas));count=0
for(n in 1:length(neigh)){
inds = which(names(keyword_dico)==neigh[n])
if(length(inds)>0){probas=probas+them_probas[inds[1],];count=count+1}
}
if(count>0){probas=probas/count}
if(sum(probas)>0){cybsecorigin=append(cybsecorigin,1-sum(probas^2))}
}
cybsecorigout=c()
#cybsecorigall=c()
for(i in 1:length(cybnodes)){
show(i)
neigh = neighbors(gcitation,v=cybnodes[i],mode="out")$name
show(neigh)
probas = rep(0,ncol(them_probas));count=0
for(n in 1:length(neigh)){
inds = which(names(keyword_dico)==neigh[n])
if(length(inds)>0){probas=probas+them_probas[inds[1],];count=count+1}
}
if(count>0){probas=probas/count}
if(sum(probas)>0){cybsecorigout=append(cybsecorigout,1-sum(probas^2))}
}
dat = data.frame(orig=c(cybsecorigin,cybsecorigout),type=c(rep("in",length(cybsecorigin),rep("out",length(cybsecorigout)))
)
)
c(cybsecorigin,cybsecorigout)
dat = data.frame(orig=c(cybsecorigin,cybsecorigout),type=c(rep("in",length(cybsecorigin)),rep("out",length(cybsecorigout)))
)
dat
g=ggplot(dat, aes(x=orig, fill=type)) + geom_density(alpha=.3)
ggplot(dat, aes(x=orig, fill=type)) + geom_density(alpha=.3)
sdat=as.tbl(dat)%>%group_by(type)%>%summarise(mean=mean(orig))
ggplot(dat, aes(x=orig, fill=type)) + geom_density(alpha=.3)+geom_vline(data=sdat, aes(xintercept=mean,  colour=cyb),linetype="dashed", size=1)
ggplot(dat, aes(x=orig, fill=type)) + geom_density(alpha=.3)+geom_vline(data=sdat, aes(xintercept=mean,  colour=type),linetype="dashed", size=1)
is.numeric("12")
as.numeric("12")
as.numeric("12mph")
is.numeric(as.numeric("12mph"))
gsub(x = "30 mph"," ","")
s=gsub(x = "30 mph"," ","")
s=gsub(x = s," ","")
s
sr=gsub(x = s," ","")
normalizedSpeed <- function(s){
if(!is.na(as.numeric(s))){return(as.numeric(s))}
sr=gsub(x = s," ","")
if(grepl("mph",sr)){return(as.numeric(gsub(x = sr,"mph",""))*1.609)}
else{return(NA)}
}
normalizedSpeed("30 mph")
normalizedSpeed("30mph")
normalizedSpeed("30")
normalizedSpeed(30)
raw <- raster(paste0(Sys.getenv("CN_HOME"),"/Data/PopulationDensity/raw/density_wgs84.tif"))
library(raster)
raw <- raster(paste0(Sys.getenv("CN_HOME"),"/Data/PopulationDensity/raw/density_wgs84.tif"))
raw
xyFromCell()
xyFromCell
xyFromCell(raw,1230)
library(RPostgreSQL)
library(rgeos)
con = dbConnect(dbDriver("PostgreSQL"), dbname="osm_simpl",user="Juste",host="localhost" )
query = dbSendQuery(con,"SELECT ST_AsText(geography) AS geom FROM links;")
data = fetch(query,n=-1)
geoms = data$geom
roads=list()
for(i in 1:length(geoms)){
r=readWKT(geoms[i])@lines[[1]];r@ID=as.character(i)
roads[[i]]=r
}
splines = SpatialLines(LinesList = roads)
plot(splines)
library(igraph)
help("difference")
real_raw = read.csv(
paste0(Sys.getenv("CN_HOME"),'/Results/Morphology/Density/Numeric/20150806_europe50km_10kmoffset_100x100grid.csv'),
sep=";"
)
real =real_raw[!is.na(real_raw[,3])&!is.na(real_raw[,4])&!is.na(real_raw[,5])&!is.na(real_raw[,6])&!is.na(real_raw[,7])&!is.na(real_raw[,8])&!is.na(real_raw[,9]),]
for(j in 1:ncol(real)){real[,j]=(real[,j]-min(real[,j]))/(max(real[,j])-min(real[,j]))}
library(RColorBrewer)
library(ggplot2)
library(MASS)
source(paste0(Sys.getenv('CN_HOME'),'/Models/Utils/R/plots.R'))
real_raw = read.csv(
paste0(Sys.getenv("CN_HOME"),'/Results/Morphology/Density/Numeric/20150806_europe50km_10kmoffset_100x100grid.csv'),
sep=";"
)
real =real_raw[!is.na(real_raw[,3])&!is.na(real_raw[,4])&!is.na(real_raw[,5])&!is.na(real_raw[,6])&!is.na(real_raw[,7])&!is.na(real_raw[,8])&!is.na(real_raw[,9]),]
real_ind = real[5*(0:(nrow(real)/5))+1,]
names(real)
indic="moran"
p = ggplot(data.frame(x=real$y,y=1-real$x,density_max=real[[indic]]),aes(x=x,y=y,colour=density_max))
p+geom_point()+xlab("")+ylab("")+labs(title=indic)+scale_colour_gradientn(colours=rev(rainbow(5)))+scale_y_continuous(breaks=NULL)+scale_x_continuous(breaks=NULL)
indic="distance"
p = ggplot(data.frame(x=real$y,y=1-real$x,density_max=real[[indic]]),aes(x=x,y=y,colour=density_max))
p+geom_point()+xlab("")+ylab("")+labs(title=indic)+scale_colour_gradientn(colours=rev(rainbow(5)))+scale_y_continuous(breaks=NULL)+scale_x_continuous(breaks=NULL)
map<-function(indic){
d=data.frame(x=real$y,y=1-real$x);d[[indic]]=real[[indic]]
p=ggplot(d,aes_string(x="x",y="y",colour=indic))
p+geom_point(shape=".",size=2)+xlab("")+ylab("")+labs(title=indic)+scale_colour_gradientn(colours=rev(rainbow(5)))+scale_y_continuous(breaks=NULL)+scale_x_continuous(breaks=NULL)
}
# multiplots
indics=c("moran","distance","entropy","slope")
plots=list();k=1
for(indic in indics){
plots[[k]]=map(indic)
k=k+1
}
multiplot(plotlist=plots,cols=2)
map<-function(indic){
d=data.frame(x=real$y,y=1-real$x);d[[indic]]=real[[indic]]
p=ggplot(d,aes_string(x="x",y="y",colour=indic))
p+geom_point(shape=".",size=1)+xlab("")+ylab("")+labs(title=indic)+scale_colour_gradientn(colours=rev(rainbow(5)))+scale_y_continuous(breaks=NULL)+scale_x_continuous(breaks=NULL)
}
# multiplots
indics=c("moran","distance","entropy","slope")
plots=list();k=1
for(indic in indics){
plots[[k]]=map(indic)
k=k+1
}
multiplot(plotlist=plots,cols=2)
names(real)
vars = c(3,4,5,6)
ccoef=c()
for(k in 2:15){
show(k)
clust = kmeans(real[,vars],k,iter.max=30)
#ccoef=append(ccoef,sum(clust$withinss/clust$size)/k)# mean cluster size
ccoef=append(ccoef,clust$tot.withinss/clust$betweenss)# clust coef
plot(real$y,1-real$x,col=clust$cluster,pch='.',cex=3,main=paste0('k=',k),xlab="",ylab="",xaxt='n',yaxt='n')
}
ccoef
vars = c(3,4,5,6)
ccoef=c()
for(k in 2:15){
show(k)
clust = kmeans(real[,vars],k,iter.max=30)
#ccoef=append(ccoef,sum(clust$withinss/clust$size)/k)# mean cluster size
ccoef=append(ccoef,clust$tot.withinss/(clust$betweenss+clust$tot.withinss))# clust coef
plot(real$y,1-real$x,col=clust$cluster,pch='.',cex=3,main=paste0('k=',k),xlab="",ylab="",xaxt='n',yaxt='n')
}
ccoef
vars = c(3,4,5,6)
ccoef=c()
par(mfrow=c(3,3))
for(k in 2:11){
show(k)
clust = kmeans(real[,vars],k,iter.max=30)
#ccoef=append(ccoef,sum(clust$withinss/clust$size)/k)# mean cluster size
withinProp=clust$tot.withinss/(clust$betweenss+clust$tot.withinss)
ccoef=append(ccoef,withinProp)# clust coef
plot(real$y,1-real$x,col=clust$cluster,pch='.',cex=3,main=paste0('k=',k,' ; withinProp=',withinProp),xlab="",ylab="",xaxt='n',yaxt='n')
}
vars = c(3,4,5,6)
ccoef=c()
par(mfrow=c(3,3))
for(k in 3:11){
show(k)
clust = kmeans(real[,vars],k,iter.max=30)
#ccoef=append(ccoef,sum(clust$withinss/clust$size)/k)# mean cluster size
withinProp=clust$tot.withinss/(clust$betweenss+clust$tot.withinss)
ccoef=append(ccoef,withinProp)# clust coef
plot(real$y,1-real$x,col=clust$cluster,pch='.',cex=3,main=paste0('k=',k,' ; withinProp=',withinProp),xlab="",ylab="",xaxt='n',yaxt='n')
}
shiny::runApp('~/Documents/ComplexSystems/CyberGeo/cybergeo20/Cybergeo20')
help("forceNetwork")
help(save)
technoprefix=paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Data/processed/classes/technoPerYear/technoProbas_')
setwd(paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Models/Semantic'))
technoprefix=paste0(Sys.getenv('CS_HOME'),'/PatentsMining/Data/processed/classes/technoPerYear/technoProbas_')
load(paste0(technoprefix,year,'_sizeTh',sizeTh,'.RData'))
year=1976
load(paste0(technoprefix,year,'_sizeTh',sizeTh,'.RData'))
sizeTh=10
load(paste0(technoprefix,year,'_sizeTh',sizeTh,'.RData'))
save(m,file=paste0(technoprefix,year,'_sizeTh',sizeTh,'_uncompressed.RData'))
save(m,file=paste0(technoprefix,year,'_sizeTh',sizeTh,'_uncompressed.RData'),compress=FALSE)
years = 1976:2012
for(year in years){
load(paste0(technoprefix,year,'_sizeTh',sizeTh,'.RData'))
save(m,file=paste0(technoprefix,year,'_sizeTh',sizeTh,'_uncompressed.RData'),compress=FALSE)
}
setwd(paste0(Sys.getenv('CS_HOME'),'/Cybergeo/cybergeo20/HyperNetwork/Models/Analysis'))
source('networkConstruction.R')
db='relevant_full_50000_eth50_nonfiltdico'
dbparams = 'relevant_full_50000_eth50_nonfiltdico_kmin0_kmax1200_freqmin50_freqmax10000_eth100'
load(paste0('probas/',dbparams,'.RData'))
load(paste0('processed/',db,'.RData'))
keyword_dico=res$keyword_dico;g=res$g;rm(res);gc()
them_probas = probas
# define comunities names
# com
thematics = communities(sub$com)
#thematics[[1]]
# define names by hand
themnames = as.character(read.csv(file=paste0('export/comunitiesnames/',dbparams,'.csv'),header=FALSE,stringsAsFactors = FALSE)[,1])
names(thematics)<-themnames
# construct kws df
kws=c()
for(i in 1:length(thematics)){
if(!is.na(names(thematics)[i])){
for(kw in thematics[[i]]){
show(c(kw,names(thematics)[i]))
kws=append(kws,c(kw,names(thematics)[i]))
}
}
}
kwdf = data.frame(matrix(kws,ncol=2,byrow=TRUE))
# load them probas
#  -> precomputed in semthem_probas
# select existing thematics
export_probas = them_probas[,!is.na(names(thematics))]
colnames(export_probas) = names(thematics)[!is.na(names(thematics))]
# need iscyb and cybindexes
# -> load from consolidated db
#export_probas = cbind(data.frame(export_probas),as.character(names(keyword_dico)))
#colnames(export_probas)[13]="ID"
load(paste0(Sys.getenv('CS_HOME'),'/Cybergeo/cybergeo20/HyperNetwork/Data/nw/citationNetwork.RData'))
cybergeo <- read.csv(paste0(Sys.getenv('CS_HOME'),'/Cybergeo/cybergeo20/Data/raw/cybergeo.csv'),colClasses = c('integer',rep('character',25)))
cyb = getCybindexes(them_probas,cybnames,cybergeo,keyword_dico)
cybid=cyb$cybid;iscyb=cyb$iscyb
export_probas = cbind(cybid,data.frame(export_probas))
names(export_probas)[1] = "CYBERGEOID"
citadjacency = get.adjacency(gcitation,sparse=TRUE)[names(keyword_dico),names(keyword_dico)]
rm(gcitation);gc()
#cybprobas = as.tbl(export_probas[export_probas$CYBERGEOID>0,])
#cybprobas[cybprobas$crime>0.1,]
#intersect(keyword_dico[[cybergeo$SCHID[cybergeo$id==4994]]],thematics[['cognitive sciences']])
#res = left_join(as.tbl(export_probas),as.tbl(cybergeo),by=c("ID","SCHID"))
# export into dbparams
# exdir=paste0('export/',dbparams)
# dir.create(exdir)
#
# write.table(export_probas,col.names = TRUE,row.names = FALSE,file = paste0(exdir,'/docprobas.csv'),sep=",")
# write.table(kwdf,col.names = FALSE,row.names = FALSE,file = paste0(exdir,'/thematics.csv'),sep=",")
#
# # export subgraph (for viz)
# keptvertices = sapply(V(sub$gg)$name,function(s){s%in%kwdf[,1]})
# gg=induced_subgraph(sub$gg,keptvertices)
# ind=1:n
kwdf
sub$com
kws = as.tbl(kwdf)
kws
kws %>% group_by(X2)
kws %>% group_by(X2) %>% summarise(l=length(X1))
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
kws[kws[,2]=='political sciences/critical geography',]
data.frame(kws[kws[,2]=='political sciences/critical geography',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='biogeography',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='economic geography',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='environnment/climate',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='environnment/climate',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='complex systems',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='physical geography',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='spatial analysis',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='microbiology',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='statistical methods',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='cognitive sciences',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='GIS',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='traffic modeling',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='traffic modeling',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='health',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='remote sensing',])
data.frame(kws[kws[,2]=='crime',])
kws %>% group_by(X2) %>% summarise(l=length(X1)) %>% arrange(l)
data.frame(kws[kws[,2]=='crime',])
library(Matrix)
probas = export_probas[,2:ncol(export_probas)]
##
# publication-level originality
originalities=apply(probas,MARGIN = 1,FUN = function(r){if(sum(r)==0){return(0)}else{return(1 - sum(r^2))}})
dat=data.frame(orig=originalities,cyb=iscyb)
sdat=as.tbl(dat)%>%group_by(cyb)%>%summarise(mean=mean(orig))
gp=ggplot(dat, aes(x=orig, fill=cyb))
gp+ geom_density(alpha=.3)+geom_vline(data=sdat, aes(xintercept=mean,  colour=cyb),linetype="dashed", size=1)
library(ggplot2)
dat=data.frame(orig=originalities,cyb=iscyb)
sdat=as.tbl(dat)%>%group_by(cyb)%>%summarise(mean=mean(orig))
gp=ggplot(dat, aes(x=orig, fill=cyb))
gp+ geom_density(alpha=.3)+geom_vline(data=sdat, aes(xintercept=mean,  colour=cyb),linetype="dashed", size=1)
