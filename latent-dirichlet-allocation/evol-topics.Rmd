---
title: "Evolution des thématiques dans les publications de Cybergéo"
csl: journal-of-biogeography.csl
date: "15 décembre 2015"
output: html_document
bibliography: papers.bib
---

```{r "ini-knitr", echo=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  #fig.height = 3,
  fig.path = "figures/",
  # Permet d'avoir un cache non dépendant du format de compilation
  cache = 1,
  cache.path = "cache/"
)
```

```{r "ini-load-functions", cache=FALSE}
library(plyr)
library(stringr)
library(lubridate)
library(dplyr)
library(koRpus)
library(stringi)
library(tidyr)
library(doParallel)
library(topicmodels)
library(slam)
library(tm)
library(ggplot2)
library(pdc)
```

```{r "ini-vars"}
nbCores <- ceiling(detectCores()/2)
registerDoParallel(cores=nbCores)
textPath <- "../../Data/cybergeo-articles-1996-2015/texts"
articlesMetaDataFile <- "cybergeo.csv"
```

```{r "ini-vars-simulation"}
models <- c("LDA")
nbrReplications <- 1
nbrFolds <- 10
k.list <- c(2, 5, 10, 20, seq(25, 36, 2), 50, 100, 200)
```

```{r "ini-vars-lda"}
k0 <- 20
```

```{r "ini-vars-analysis"}
nbrMotsParThematique <- 20
```

```{r "ini-vars-pdc"}
ts.lag.max <- 8 # Pour le graphique d'entropie
nbr.dimension <- 3 # Évolution déduite sur une fenêtre de 3 temps
ts.lag <- 1 # Décalage temporel permis
```

## Introduction 

```{r "load-cybergeo-metadata"}
#-- Lecture des méta-données du corpus
articles <- read.table(
  articlesMetaDataFile, 
  sep = ";", 
  quote = "", 
  comment.char = "", 
  header = TRUE
) %>% 
  tbl_df() %>%
  rename(
    titre = title_en, 
    auteurs = authors
  ) %>%
  select(id, date, langue, auteurs, titre) %>%
  mutate(
    langue = tolower(langue),
    date = as.Date(date)
  ) %>%
  arrange(date, auteurs)
```

[Cybergeo](https://cybergeo.revues.org/) est une revue scientifique européenne de géographie. Depuis sa création en 1996, `r nrow(articles)` documents ont été publiés. L'accès aux textes intégraux de ce large *corpus* sur ce temps long nous permet d'appréhender l'évolution des thématiques abordées par les Géographes : Quelles thématiques sont récurrentes chez les auteurs de Cybergeo ? Quelle est l'évolution de ces thématiques au cours du temps ?

## Méthode

Afin de répondre à ce questionnement, nous proposons tout d'abord d'identifier des thématiques dans le *corpus*, puis d'identifier des typologies d'évolution de ces thématiques au cours du temps.

Afin d'identifier nos thématiques, nous proposons d'utiliser l'Allocation de Dirichlet latente, ou LDA [@Blei:2003tn]. La méthode suppose que les mots sont des observations collectées dans les documents du corpus, que chaque document est un mélange d'un faible nombre de thématiques et que chaque mot est attribuable à l'une des thématiques du document. La méthode nécessite au préalable de collecter les mots de chaque document et de les normaliser, puis d'estimer le nombre de thématiques le plus adapté à notre corpus. Chaque mot est ensuite tiré aléatoirement pour constituer des thématiques. Il en résulte une liste des mots les plus probables par thématique et une probabilité de chaque thématique par document.

Une fois les thématiques identifiées, nous réaliserons une classification des séries temporelles d'évolution des thématiques (cf. graphique ci-dessous pour la série temporelle des publications de Cybergeo). Nous utiliserons la méthode dite de *Permutation distribution clustering* [@Brandmaier:2011vh]. Il s'agit de réduire les séries temporelles de présence des thématiques à une évolution relative (augmentation, diminution) en tenant compte d'un possible décalage temporel entre séries temporelles. Il en résulte une classification des séries temporelles selon leur similarité.

```{r "plot-ts-publications", dependson=c("load-cybergeo-metadata")}
ts.articles <- articles %>%
  select(date) %>%
  mutate(date = as.Date(cut(date, breaks = "1 month"))) %>%
  group_by(date) %>%
  summarise(counts = n())
ggplot(
  ts.articles, 
  aes(x = date, y = counts)
  ) + 
  geom_line() + 
  scale_x_date()+
  labs(
    y = "Nombre de documents", 
    x = "Années", 
    title = "Évolution mensuelle des publications\nde la revue Cybergeo"
  )
```

## Présentation du corpus

Ce corpus contient des documents de différents types : des renvois, des bibliographies, des comptes-rendus de lecture, des articles courts et longs. Cette typologie se retrouve dans la distibution des nombres de mots par document (cf. figure ci-après). Ces documents sont écrits dans différentes langues, la langue française étant majoritaire (`r articles %>% filter(articles$langue == "fr") %>% nrow` documents, cf. graphique ci-après).

```{r "tagging-pre", dependson=c("load-cybergeo-metadata")}
files <- articles %>%
  filter(langue == "fr") %>%
  select(id) %>%
  mutate(
    name = paste(id, "_text.txt", sep = ""),
    path = paste(textPath, name, sep = "/"),
    size = file.size(path)
  ) %>%
  filter(size > 400)
```

```{r "tagging", dependson=c("tagging-pre")}
tag.corpus <- foreach(id = files$id) %dopar% {
  file <- files$path[files$id == id]
  tagged.text <- treetag(
    file = file,
    treetagger = "manual",
    lang = "fr",
    encoding = "UTF-8",
    TT.tknz = FALSE,
    TT.options = list(path = "~/treetagger", preset = "fr-utf8"),
    stopwords = tm::stopwords("fr"),
    stemmer = SnowballC::wordStem
  )
  return(list(id = id, tagger = tagged.text))
}
```

```{r "lemmatisation", dependson=c("tagging")}
d0 <- lapply(tag.corpus, function(x) {
  id <- x$id
  taggedText(x$tagger) %>%
    tbl_df() %>%
    # Récupération des noms communs et propros, ainsi que des verbes
    filter(str_sub(tag,1,3) %in% c("NAM","NOM","VER")) %>%
    # Si aucun lemme n'est défini (cas des noms propres), alors prendre le mot initial
    mutate(lemmes = ifelse(lemma == "<unknown>", token, lemma)) %>%
    # Suppression des nombres et des points
    mutate(lemmes = gsub("[0-9.]", "", lemmes, perl = TRUE)) %>%
    # Suppression des termes de moins de 3 caractères
    filter(nchar(lemmes) >= 3) %>% 
    # Calcul des fréquences de termes
    group_by(lemmes) %>%
    summarise(counts = n()) %>%
    mutate(DOCID = id)
}) %>%
  ldply %>%
  tbl_df %>%
  arrange(lemmes)
```

```{r "dtm", results='hide', dependson=c("lemmatisation","tagging-pre")}
dict0 <- unique(d0$lemmes)
d1 <- d0 %>%
  mutate(lemidx = match(lemmes, dict0) - 1)
l5 <- d1 %>% 
  select(lemidx) %>%
  group_by(lemidx) %>%
  summarise(nbDoc = n()) %>%
  # Suppression des termes apparaissant dans moins de 5 documents
  # Car je m'intéresse à l'évolution des thèmes, pas à l'apparition
  # de thèmes qui ne trouveront pas d'écho ailleurs
  filter(nbDoc >= 5) %>%
  select(-nbDoc)
d2 <- l5 %>%
  left_join(d1)

#-- Construction de la matrice document-termes
vocabulaire <- unique(d2$lemmes)
d3 <- d2 %>%
  mutate(termidx = match(lemmes, vocabulaire) - 1)
doc <- lapply(files$id, function(id){
   f0 <- d3 %>%
    filter(DOCID == id) %>%
    select(-lemmes, -DOCID) %>%
    arrange(termidx)
  rbind(f0$termidx, f0$counts)
})
dtm <- ldaformat2dtm(doc, vocabulaire)

#-- Calcul du poids TF-IDF
dim(dtm)
summary(col_sums(dtm))
term_tfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
summary(term_tfidf)
dtm <- dtm[, term_tfidf >= median(term_tfidf)]
dtm <- dtm[row_sums(dtm) > 0,]
summary(col_sums(dtm))
dim(dtm)
```

```{r "corpus-vars", dependson=c("tagging","dtm")}
mots <- ldply(lapply(tag.corpus, function(x) { x$tagger@TT.res }))
nbrTokens <- format(nrow(mots), big.mark = " ")
nbrChars <- ldply(lapply(tag.corpus, function(x) { x$tagger@desc$words }))
nbrMots <- format(sum(nbrChars$V1), big.mark = " ")
nbrLemmes <- d0 %>% nrow %>% format(big.mark = " ")
nbrLemmes0 <- dict0 %>% length %>% format(big.mark = " ")
nbrLemmes1 <- vocabulaire %>% length %>% format(big.mark = " ")
nbrLemmes2 <- dim(dtm)[2]
```

```{r "plot-nb-mots", dependson=c("corpus-vars")}
ggplot(nbrChars, aes(x = V1)) + 
  geom_histogram() +
  labs(
    x = "Nombre de mots",
    y = "Nombre de documents",
    title = "Distribution du nombre de mots\npar publication de la revue Cybergeo"
  )
```

```{r "plot-languages", dependson=c("load-cybergeo-metadata")}
t <- table(articles$langue)
langues <- data_frame(
  lang = factor(names(t)), 
  count = as.numeric(t)
) %>%
  arrange(count) %>%
  mutate(
    m = max(count),
    v = ifelse(count < m/2, -0.5, 1.5)
  )
ggplot(langues, aes(x = lang, y = count, fill = lang, label = count, vjust = v)) + 
  geom_bar(stat = "identity") +
  #coord_flip() +
  labs(
    y = "Nombre de documents", 
    x = "Langues", 
    title = "Distribution de la langue d'écriture\ndes publications de la revue Cybergeo",
    fill = "Langues"
  ) +
  geom_text()
```

## Normalisation des textes

Nous traiterons les documents en langue française. Les bibliographies et les renvois, ne constituant pas un texte en soi, ne seront pas traités. Notre corpus se réduit maintenant à `r nrow(files)` documents composés de `r nbrMots` mots.

Afin de normaliser ces textes, vu le nombre de documents, nous devons faire appel à un traitement automatique de la langue. Nous utiliserons pour cela un programme pour, tout d'abord, identifier les éléments du discours, puis pour les réduire à leur forme canonique (lemmatisation). Tout d'abord, nous identifions les éléments du discours et leur nature : abréviation, adjectif, adverbe, déterminant, interjection, conjonction, nom commun, nom propre, pronom, préposition, ponctuation, symbole, verbe, temps de conjugaison. Notre corpus se compose ainsi `r nbrTokens` éléments de discours. Ensuite, nous réduisons ces éléments de discours à leur lemmes, c'est à dire leur forme canonique : les noms au pluriel seront mis au singulier, les verbes conjugués seront mis à l'infinitif, *etc*. Nous retiendrons pour notre analyse que les éléments les plus signifiants, soit les noms communs, les noms propres et les verbes ; chacun possédant au moins trois lettres. À ce stade, notre corpus se compose de `r nbrLemmes` lemmes, dont `r nbrLemmes0` sont uniques. Comme nous nous intéressons à l'évolution de thématiques au sein de plusieurs articles, nous ne retiendrons que les mots apparaissant au moins dans cinq documents. Notre corpus se réduit ainsi à `r nbrLemmes1` lemmes uniques. Pour finaliser notre processus, nous attribuons un poids à chacun de ces lemmes. Nous utilisons pour cela la méthode de calcul TF-IDF qui permet de cerner les termes les plus significatifs dans un corpus [@SparckJones:1972vw]. Nous ne retiendrons que la moitié des lemmes les plus significatifs, soit `r nbrLemmes2` lemmes uniques.

## Identification des thématiques

Pour estimer le nombre de thématiques approprié à notre *corpus*, nous allons tester l'adéquation de différentes valeurs. Nous testerons la capacité à rendre compte des thématiques d'un corpus de test constitué par 10% des documents d'après les thématiques identifiées sur le reste du corpus. Nous effectuerons cette validation croisée 10 fois, permettant ainsi de tester l'ensemble du *corpus*. Deux indicateurs - la perplexité et l'entropie - nous permettront de sélectionner le nombre de thématiques adéquat.

Nous constatons dans les graphiques ci-dessous que :

* la perplexité minimale est atteinte avec un nombre de thématiques croissant. Les thématiques traitées dans Cybergeo sont donc très diverses.
* l'entropie maximale est toutefois atteinte aux environs de 20 à 35 thématiques, ce qui signifie que les thématiques identifiées sont assez bien réparties entre les documents.

```{r "simulation", dependson=c("dtm","ini-vars-simulation")}
simulation <- function(id) {
  model <- prog$model[prog$id == id]
  k <- prog$k[prog$id == id]
  rep <- prog$rep[prog$id == id]
  fold <- prog$fold[prog$id == id]
  FILE <- paste("VEM_", k, "_", fold, ".rda", sep = "")
  training <- LDA(texts[folding != fold,], k = k)
  testing <- LDA(texts[folding == fold,], model = training, control = list(estimate.beta = FALSE))
  d <- data.frame(
    id = id,
    rep = rep,
    model = model,
    k = k,
    fold = fold,
    alpha = training@alpha,
    perplexity = perplexity(testing, texts[folding == fold,], use_theta = FALSE),
    entropie = mean(apply(posterior(training)$topics, 1, function(z) - sum(z * log(z))))
  )
  return(d)
}

texts <- dtm
folding <- sample(1:nbrFolds, nrow(texts), replace = TRUE)
prog <- expand.grid(
  rep = 1:nbrReplications,
  model = models,
  k = k.list, 
  fold = 1:nbrFolds
)
prog$id <- 1:nrow(prog)

result <- rbind.fill(foreach(id = prog$id) %dopar% simulation(id))
```

```{r "plot-perplexity", dependson=c("simulation")}
# Perplexity
ggplot(result, aes(k, perplexity)) +
  geom_point() +
  labs(
    x = "Nombre de thématiques",
    y = "Perplexité",
    title = "Evolution de la perplexité\nselon le nombre de thématiques\npendant la validation croisée"
  )
```

```{r "plot-alpha", eval = FALSE, dependson=c("simulation")}
# Alpha
# The lower α the higher is the percentage of documents which are assigned to one single topic with a high probability
# Cela implique que les documents consistent un nombre limité de thèmes
ggplot(result, aes(k, alpha)) +
  geom_point() + #+ xlim(0,100)
  facet_wrap(~ rep)
```

```{r "plot-entropy", dependson=c("simulation")}
# Entropie
# Higher values indicate that the topic distributions are more evenly spread over the topics.
g.ent <- ggplot(result, aes(k, entropie)) +
  geom_point() +
  labs(
    x = "Nombre de thématiques",
    y = "Entropie"
  )
g.ent + labs(title = "Evolution de l'entropie\nselon le nombre de thématiques\npendant la validation croisée")
```

```{r "plot-entropy-zoom", dependson=c("simulation")}
# Entropie
# Higher values indicate that the topic distributions are more evenly spread over the topics.
g.ent + xlim(10, 50) + ylim(0.65, 0.9) + labs(title = "Evolution de l'entropie\nselon le nombre de thématiques\npendant la validation croisée (zoom)")
```

```{r "plot-mean-entropie", eval=FALSE, dependson=c("simulation")}
res.mean <- result %>%
  group_by(k) %>%
  summarise(
    alpha = mean(alpha),
    perplexity = mean(perplexity),
    entropie = mean(entropie)
  )
ggplot(res.mean, aes(k, entropie)) +
  geom_point()
```

## Cas avec `r k0` thématiques

### Analyse des thématiques identifiées

Nous présentons ci-dessous les `r nbrMotsParThematique` mots, les plus significatifs, qui ont été alloués à chacun des `r k0` thèmes ; puis un graphique montrant la probabilité d'allocation d'une thématique par document (un document est représenté par un trait fin vertical).

```{r "topics", dependson=c("dtm","ini-vars-lda")}
# Importance de chaque thème (+ CAH)
# Distribution des probabilités de chaque thème dans les documents (ou liste des 3 premiers titres par thème)
# Importance de chaque thème dans chaque documents
# Evolution de chaque théme dans le temps (probabilité)
# Clustering de séries temporelles
# Sankey diagram
# Graphes des thèmes :
# - un noeud = 1 document représenté par le mot le plus représentatif du thème)
# - un lien (un thème commun)
# - Ajouter peu à peu les thèmes majeurs, secondaires, etc.

texts <- dtm
a <- result %>%
  filter(k == k0) %>%
  group_by(model) %>%
  summarise(alpha = mean(alpha))
alpha <- a$alpha
model <- LDA(texts, k = k0, control = list(alpha = alpha))
```

```{r "first-analysis", dependson=c("topics","ini-vars-analysis")}
# Trois premiers thèmes dans chaque document
#topics(model, 3)
# Termes de chaque thème
thematique <- terms(model, nbrMotsParThematique) %>%
  as.data.frame() %>%
  tbl_df() %>%
  gather("theme", "mot")
themes.uniques <- thematique$theme %>% unique
thematique <- thematique %>%
  mutate(Thèmes = match(theme, themes.uniques)) %>%
  select(-theme) %>%
  group_by(Thèmes) %>%
  summarise(Mots = paste(mot, collapse = ", "))
kable(thematique)
```

```{r "plot-alloc-pre", dependson=c("topics","tagging-pre")}
# Graphique de la répartition de la probabilité de chaque thème dans chaque document
clustering.order <- function(df) {
  d <- dist(p, method = "euclidean")
  h <- hclust(d, method = "ward.D2")
  return(h)
}

p <- posterior(model)$topics %>%
  as.data.frame()
row.names(p) <- files$id
doc.clust <- clustering.order(p)
doc.order <- doc.clust$order

p <- posterior(model)$topics %>%
  as.data.frame() %>%
  tbl_df()
p$id <-  files$id
p <- p %>%
  gather("topic", "p", 1:k0) %>%
  spread(id, p)
row.names(p) <- p$topic
topic.clust <- clustering.order(p)
topic.order <- topic.clust$order

p <- posterior(model)$topics %>%
  as.data.frame() %>%
  tbl_df()
p$id <- files$id
p <- p %>%
  gather("topic", "p", 1:k0)
p1 <- p
p$topic <- factor(p$topic, levels = (1:k0)[topic.order])
p$id <- factor(files$id, levels = files$id[doc.order])
```

```{r "plot-clust-doc", dependson=c("plot-alloc-pre"), eval=FALSE}
plot(doc.clust)
```

```{r "plot-clust-topic", dependson=c("plot-alloc-pre"), eval=FALSE}
plot(topic.clust)
```

```{r "plot-alloc", dependson=c("plot-alloc-pre"), fig.height=5}
ggplot(p, aes(x = id, y = topic, fill = p)) +
  geom_tile() +
  labs(
    x = "Documents", 
    y = "Thématiques", 
    title = "Probabilité d'allocation d'une thématique\npar document de la revue Cybergeo",
    fill = "Probabilité"
  ) +
  theme(
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  )
```

```{r "other-analysis", eval=FALSE, cache=FALSE}
# Taille des groupes
table(groupe)
g1 <- groupe[groupe == 2] %>% names
articles %>%
  filter(id %in% g1) %>%
  select(id, titre)

p$id <- files$id
p <- p %>%
  gather("topic", "p", 1:k0)

articles %>% filter(id == 6085)
p %>% filter(id == 6085) %>% arrange(desc(p)) %>% mutate(p = round(p,digits = 2))
terms(model, 20)[,32]
terms(model, 20)[,7]
terms(model, 20)[,21]

groupe <- cutree(h, k = 43)
# Taille des groupes
table(groupe)
g1 <- groupe[groupe == 2] %>% names
articles %>%
  filter(id %in% g1) %>%
  select(id, titre)
```

### Analyse des trajectoires des thématiques

```{r "calc-evol-nbr-docs-par-themes", cache=FALSE}
freq <- p1 %>%
  left_join(articles) %>%
  mutate(annee = str_sub(date, 1, 4)) %>%
  rename(topic_id = topic) %>%
  group_by(annee, topic_id) %>%
  summarise(counts = sum(p)) %>%
  arrange(annee, topic_id)
```

```{r "calc-heuristic-and-clust", cache=FALSE}
m <- freq %>%
  mutate(counts = as.numeric(counts)) %>%
  spread(topic_id, counts) %>%
  select(-annee) %>%
  as.matrix()
mine <- entropyHeuristic(m, t.max = ts.lag.max)
#clust <- pdclust(m, m = nbr.dimension, t = ts.lag)
clust <- pdclust(m)
```

Nous recherchons maintenant à classer les séries temporelles selon leur similarité. Pour cela, nous étudions les évolutions sur `r mine$m` années consécutives, en permettant un retard entre deux séries de `r mine$t` années. Un premier graphique montre la classification obtenue ; le deuxième l'évolution temporelle sur 20 ans des thématiques classées selon leurs similarités.

```{r "plot-entropie", eval=FALSE}
plot(mine)
```

```{r "plot-cah-sequence", cache=FALSE}
plot(clust, labels = colnames(m))
```

```{r "plot-cah-ts", eval=FALSE}
plot(clust)
```

```{r "plot-evol-nbr-docs-par-themes", cache=FALSE, fig.height=7.5}
freq$topic_id <- factor(freq$topic_id, levels = colnames(m)[clust$order])
ggplot(freq, aes(annee, counts)) + 
  geom_bar(stat = "identity") + 
  facet_grid(topic_id ~ ., scales = "free_y", space = "free_y") +
  labs(
    x= "Années",
    y = "Nombre de documents",
    title = "Évolution du nombre de documents par thématique"
  ) +
  scale_x_discrete(labels=c("96", "97", "98", "99", "00", "01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15")) +
  scale_y_continuous(breaks=c(5,10,15))

```



```{r "topics-35"}
k0 <- 35
a <- result %>%
  filter(k == k0) %>%
  group_by(model) %>%
  summarise(alpha = mean(alpha))
alpha <- a$alpha
model <- LDA(texts, k = k0, control = list(alpha = alpha))
```

## Cas avec `r k0` thématiques

### Analyse des thématiques identifiées

Nous présentons ci-dessous les `r nbrMotsParThematique` mots, les plus significatifs, qui ont été alloués à chacun des `r k0` thèmes ; puis un graphique montrant la probabilité d'allocation d'une thématique par document (un document est représenté par un trait fin vertical).

```{r "first-analysis-35", cache=FALSE}
# Trois premiers thèmes dans chaque document
#topics(model, 3)
# Termes de chaque thème
thematique <- terms(model, nbrMotsParThematique) %>%
  as.data.frame() %>%
  tbl_df() %>%
  gather("theme", "mot")
themes.uniques <- thematique$theme %>% unique
thematique <- thematique %>%
  mutate(Thèmes = match(theme, themes.uniques)) %>%
  select(-theme) %>%
  group_by(Thèmes) %>%
  summarise(Mots = paste(mot, collapse = ", "))
kable(thematique)
```

```{r "plot-alloc-pre-35", cache=FALSE}
# Graphique de la répartition de la probabilité de chaque thème dans chaque document
clustering.order <- function(df) {
  d <- dist(p, method = "euclidean")
  h <- hclust(d, method = "ward.D2")
  return(h)
}

p <- posterior(model)$topics %>%
  as.data.frame()
row.names(p) <- files$id
doc.clust <- clustering.order(p)
doc.order <- doc.clust$order

p <- posterior(model)$topics %>%
  as.data.frame() %>%
  tbl_df()
p$id <-  files$id
p <- p %>%
  gather("topic", "p", 1:k0) %>%
  spread(id, p)
row.names(p) <- p$topic
topic.clust <- clustering.order(p)
topic.order <- topic.clust$order

p <- posterior(model)$topics %>%
  as.data.frame() %>%
  tbl_df()
p$id <- files$id
p <- p %>%
  gather("topic", "p", 1:k0)
p1 <- p
p$topic <- factor(p$topic, levels = (1:k0)[topic.order])
p$id <- factor(files$id, levels = files$id[doc.order])
```

```{r "plot-alloc-35", cache=FALSE, fig.height=6}
ggplot(p, aes(x = id, y = topic, fill = p)) +
  geom_tile() +
  labs(
    x = "Documents", 
    y = "Thématiques", 
    title = "Probabilité d'allocation d'une thématique\npar document de la revue Cybergeo",
    fill = "Probabilité"
  ) +
  theme(
    axis.text.x = element_blank(), 
    axis.ticks.x = element_blank()
  )
```

### Analyse des trajectoires des thématiques

```{r "calc-evol-nbr-docs-par-themes-35", cache=FALSE}
freq <- p1 %>%
  left_join(articles) %>%
  mutate(annee = str_sub(date, 1, 4)) %>%
  rename(topic_id = topic) %>%
  group_by(annee, topic_id) %>%
  summarise(counts = sum(p)) %>%
  arrange(annee, topic_id)
```

```{r "calc-heuristic-and-clust-35", cache=FALSE}
m <- freq %>%
  mutate(counts = as.numeric(counts)) %>%
  spread(topic_id, counts) %>%
  select(-annee) %>%
  as.matrix()
mine <- entropyHeuristic(m, t.max = ts.lag.max)
#clust <- pdclust(m, m = nbr.dimension, t = ts.lag)
clust <- pdclust(m)
```

Nous recherchons maintenant à classer les séries temporelles selon leur similarité. Pour cela, nous étudions les évolutions sur `r mine$m` années consécutives, en permettant un retard entre deux séries de `r mine$t` années. Un premier graphique montre la classification obtenue ; le deuxième l'évolution temporelle sur 20 ans des thématiques classées selon leurs similarités.

```{r "plot-entropie-35", eval=FALSE}
plot(mine)
```

```{r "plot-cah-sequence-35", cache=FALSE}
plot(clust, labels = colnames(m))
```

```{r "plot-cah-ts-35", eval=FALSE}
plot(clust)
```

```{r "plot-evol-nbr-docs-par-themes-35", cache=FALSE, fig.height=9.5}
freq$topic_id <- factor(freq$topic_id, levels = colnames(m)[clust$order])
ggplot(freq, aes(annee, counts)) + 
  geom_bar(stat = "identity") + 
  facet_grid(topic_id ~ ., scales = "free_y", space = "free_y") +
  labs(
    x= "Années",
    y = "Nombre de documents",
    title = "Évolution du nombre de documents par thématique"
  ) +
  scale_x_discrete(labels=c("96", "97", "98", "99", "00", "01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15")) +
  scale_y_continuous(breaks=c(5,10,15))
```


## Bibliographie

